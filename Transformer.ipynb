{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50,), dtype=float32, numpy=\n",
       "array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,\n",
       "       13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25.,\n",
       "       26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38.,\n",
       "       39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49.],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(50, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tf.newaxis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50, 1), dtype=float32, numpy=\n",
       "array([[ 0.],\n",
       "       [ 1.],\n",
       "       [ 2.],\n",
       "       [ 3.],\n",
       "       [ 4.],\n",
       "       [ 5.],\n",
       "       [ 6.],\n",
       "       [ 7.],\n",
       "       [ 8.],\n",
       "       [ 9.],\n",
       "       [10.],\n",
       "       [11.],\n",
       "       [12.],\n",
       "       [13.],\n",
       "       [14.],\n",
       "       [15.],\n",
       "       [16.],\n",
       "       [17.],\n",
       "       [18.],\n",
       "       [19.],\n",
       "       [20.],\n",
       "       [21.],\n",
       "       [22.],\n",
       "       [23.],\n",
       "       [24.],\n",
       "       [25.],\n",
       "       [26.],\n",
       "       [27.],\n",
       "       [28.],\n",
       "       [29.],\n",
       "       [30.],\n",
       "       [31.],\n",
       "       [32.],\n",
       "       [33.],\n",
       "       [34.],\n",
       "       [35.],\n",
       "       [36.],\n",
       "       [37.],\n",
       "       [38.],\n",
       "       [39.],\n",
       "       [40.],\n",
       "       [41.],\n",
       "       [42.],\n",
       "       [43.],\n",
       "       [44.],\n",
       "       [45.],\n",
       "       [46.],\n",
       "       [47.],\n",
       "       [48.],\n",
       "       [49.]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(50, dtype=tf.float32)[:, tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 128), dtype=float32, numpy=\n",
       "array([[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
       "         11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
       "         22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,\n",
       "         33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,\n",
       "         44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,\n",
       "         55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,\n",
       "         66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,\n",
       "         77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,\n",
       "         88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,\n",
       "         99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,\n",
       "        110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,\n",
       "        121., 122., 123., 124., 125., 126., 127.]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(128, dtype=tf.float32)[tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 1, 2, 3, 4],\n",
       "       [0, 2, 4, 6, 8]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(3).reshape(3,1)\n",
    "b = np.arange(5).reshape(1,5)\n",
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 128)\n",
      "(50, 1)\n",
      "tf.Tensor(\n",
      "[ 1.41120002e-01  5.17305732e-01  7.78272510e-01  9.29644823e-01\n",
      "  9.93253171e-01  9.93967772e-01  9.53634441e-01  8.89167428e-01\n",
      "  8.12648892e-01  7.32186019e-01  6.52903974e-01  5.77821970e-01\n",
      "  5.08536100e-01  4.45719600e-01  3.89470309e-01  3.39546382e-01\n",
      "  2.95520216e-01  2.56876916e-01  2.23075420e-01  1.93584546e-01\n",
      "  1.67903304e-01  1.45571157e-01  1.26171768e-01  1.09333232e-01\n",
      "  9.47260931e-02  8.20602104e-02  7.10812137e-02  6.15667887e-02\n",
      "  5.33230826e-02  4.61813621e-02  3.99949774e-02  3.46365310e-02\n",
      "  2.99954992e-02  2.59760078e-02  2.24949289e-02  1.94802172e-02\n",
      "  1.68694388e-02  1.46085061e-02  1.26505569e-02  1.09550050e-02\n",
      "  9.48669109e-03  8.21516663e-03  7.11406162e-03  6.16053585e-03\n",
      "  5.33481315e-03  4.61976323e-03  4.00055340e-03  3.46433907e-03\n",
      "  2.99999560e-03  2.59789010e-03  2.24968069e-03  1.94814370e-03\n",
      "  1.68702309e-03  1.46090204e-03  1.26508914e-03  1.09552208e-03\n",
      "  9.48683242e-04  8.21525755e-04  7.11412111e-04  6.16057427e-04\n",
      "  5.33483806e-04  4.61977907e-04  4.00056451e-04  3.46434594e-04\n",
      " -9.89992499e-01 -8.55800688e-01 -6.27926707e-01 -3.68456870e-01\n",
      " -1.15966164e-01  1.09672695e-01  3.00967306e-01  4.57582027e-01\n",
      "  5.82753658e-01  6.81104720e-01  7.57440686e-01  8.16162825e-01\n",
      "  8.61040652e-01  8.95172656e-01  9.21038985e-01  9.40589309e-01\n",
      "  9.55336511e-01  9.66444135e-01  9.74801183e-01  9.81083572e-01\n",
      "  9.85803485e-01  9.89347756e-01  9.92008388e-01  9.94005144e-01\n",
      "  9.95503366e-01  9.96627390e-01  9.97470558e-01  9.98102963e-01\n",
      "  9.98577297e-01  9.98933077e-01  9.99199867e-01  9.99399960e-01\n",
      "  9.99550045e-01  9.99662578e-01  9.99746978e-01  9.99810219e-01\n",
      "  9.99857724e-01  9.99893308e-01  9.99919951e-01  9.99939978e-01\n",
      "  9.99954998e-01  9.99966264e-01  9.99974668e-01  9.99981046e-01\n",
      "  9.99985754e-01  9.99989331e-01  9.99992013e-01  9.99993980e-01\n",
      "  9.99995530e-01  9.99996603e-01  9.99997497e-01  9.99998093e-01\n",
      "  9.99998569e-01  9.99998927e-01  9.99999225e-01  9.99999404e-01\n",
      "  9.99999523e-01  9.99999642e-01  9.99999762e-01  9.99999821e-01\n",
      "  9.99999881e-01  9.99999881e-01  9.99999940e-01  9.99999940e-01], shape=(128,), dtype=float32)\n",
      "(1, 50, 128)\n"
     ]
    }
   ],
   "source": [
    "i=tf.range(128, dtype=tf.float32)[tf.newaxis, :]\n",
    "angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(128, tf.float32))\n",
    "# print(angles)\n",
    "print(angles.shape)\n",
    "\n",
    "position=tf.range(50, dtype=tf.float32)[:, tf.newaxis]\n",
    "# print(position)\n",
    "print(position.shape)\n",
    "angle_rads = angles*position   # (50,1)(1,128) (50,128)\n",
    "# print(angle_rads)\n",
    "# print(angle_rads[:, 0::2])\n",
    "sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "# print(sines)\n",
    "\n",
    "cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "# print(cosines)\n",
    "\n",
    "pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "print(pos_encoding[3])\n",
    "pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "print(pos_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "\n",
    "        # 배열의 짝수 인덱스(2i)에는 사인 함수 적용\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "\n",
    "        # 배열의 홀수 인덱스(2i+1)에는 코사인 함수 적용\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "#         print(pos_encoding.shape)\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_pos_encoding = PositionalEncoding(40, 512)\n",
    "# print(sample_pos_encoding.pos_encoding[0,10,10])\n",
    "\n",
    "\n",
    "plt.pcolormesh(sample_pos_encoding.pos_encoding.numpy()[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    # query 크기 : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    # key 크기 : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
    "    # value 크기 : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
    "    # padding_mask : (batch_size, 1, 1, key의 문장 길이)\n",
    "\n",
    "    # Q와 K의 곱. 어텐션 스코어 행렬.\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "#     print(matmul_qk)\n",
    "    # 스케일링\n",
    "    # dk의 루트값으로 나눠준다.\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "#     print(logits)\n",
    "\n",
    "    # 마스킹. 어텐션 스코어 행렬의 마스킹 할 위치에 매우 작은 음수값을 넣는다.\n",
    "    # 매우 작은 값이므로 소프트맥스 함수를 지나면 행렬의 해당 위치의 값은 0이 된다.\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # 소프트맥스 함수는 마지막 차원인 key의 문장 길이 방향으로 수행된다.\n",
    "    # attention weight : (batch_size, num_heads, query의 문장 길이, key의 문장 길이)\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "#     print(attention_weights)\n",
    "    \n",
    "    # output : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "#     print(output)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
      "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_out, temp_attn = scaled_dot_product_attention(temp_q, temp_k, temp_v, None)\n",
    "print(temp_attn) # 어텐션 분포(어텐션 가중치의 나열)\n",
    "print(temp_out) # 어텐션 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.  0.  0.5 0.5]\n",
      " [0.  1.  0.  0. ]\n",
      " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[550.    5.5]\n",
      " [ 10.    0. ]\n",
      " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
    "temp_out, temp_attn = scaled_dot_product_attention(temp_q, temp_k, temp_v, None)\n",
    "print(temp_attn) # 어텐션 분포(어텐션 가중치의 나열)\n",
    "print(temp_out) # 어텐션 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[  0.   0.   0.]\n",
      "   [  0. 100. 100.]\n",
      "   [  0. 100. 200.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0. 200. 200.]\n",
      "   [  0. 200. 400.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0. 300. 300.]\n",
      "   [  0. 300. 600.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0. 400. 400.]\n",
      "   [  0. 400. 800.]]]], shape=(1, 4, 3, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[    0.     0.]\n",
      "   [ 1000.  2000.]\n",
      "   [ 2000.  3000.]]\n",
      "\n",
      "  [[    0.     0.]\n",
      "   [ 4000.  8000.]\n",
      "   [ 8000. 12000.]]\n",
      "\n",
      "  [[    0.     0.]\n",
      "   [ 9000. 18000.]\n",
      "   [18000. 27000.]]\n",
      "\n",
      "  [[    0.     0.]\n",
      "   [16000. 32000.]\n",
      "   [32000. 48000.]]]], shape=(1, 4, 3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_q = tf.constant([[[[0, 0], [0, 10], [10, 10]],\n",
    "                       [[0, 0], [0, 10], [10, 10]],\n",
    "                       [[0, 0], [0, 10], [10, 10]],\n",
    "                       [[0, 0], [0, 10], [10, 10]]]],dtype=tf.float32)  # (1,4,3,2)\n",
    "temp_k = tf.constant([[[[0, 0], [0, 10], [10, 10]],\n",
    "                       [[0, 0], [0, 20], [20, 20]],\n",
    "                       [[0, 0], [0, 30], [30, 30]],\n",
    "                       [[0, 0], [0, 40], [40, 40]]]],dtype=tf.float32)  # (1,4,3,2)\n",
    "temp_v = tf.constant([[[[0, 0], [0, 10], [10, 10]],\n",
    "                       [[0, 0], [0, 20], [20, 20]],\n",
    "                       [[0, 0], [0, 30], [30, 30]],\n",
    "                       [[0, 0], [0, 40], [40, 40]]]],dtype=tf.float32)  # (1,4,3,2)\n",
    "\n",
    "matmul_qk = tf.matmul(temp_q, temp_k, transpose_b=True)\n",
    "print(matmul_qk) # (1, 4, 3, 3)\n",
    "output = tf.matmul(matmul_qk, temp_v)  # (3,3)(3,2)\n",
    "print(output)\n",
    "# temp_out, temp_attn = scaled_dot_product_attention(temp_q, temp_k, temp_v, None)\n",
    "# print(temp_attn) # 어텐션 분포(어텐션 가중치의 나열)\n",
    "# print(temp_out) # 어텐션 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        \n",
    "        print('MultiHeadAttention.__init__()')\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        # d_model을 num_heads로 나눈 값.\n",
    "        # 논문 기준 : 64\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        # WQ, WK, WV에 해당하는 밀집층 정의\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        # WO에 해당하는 밀집층 정의\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  # num_heads 개수만큼 q, k, v를 split하는 함수\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        print('MultiHeadAttention.call()')\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "            'value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "        \n",
    "        print('batch_size=', batch_size)\n",
    "        print('query=', query)\n",
    "        print('key=', batch_size)\n",
    "        print('value=', value)\n",
    "        \n",
    "\n",
    "        # 1. WQ, WK, WV에 해당하는 밀집층 지나기\n",
    "        # q : (batch_size, query의 문장 길이, d_model)\n",
    "        # k : (batch_size, key의 문장 길이, d_model)\n",
    "        # v : (batch_size, value의 문장 길이, d_model)\n",
    "        # 참고) 인코더(k, v)-디코더(q) 어텐션에서는 query 길이와 key, value의 길이는 다를 수 있다.\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # 2. 헤드 나누기\n",
    "        # q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "        # k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
    "        # v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
    "        \n",
    "        print('before', query) # (1, 40, 128)\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "        print('after', query)  # (1, 4, 40, 32)\n",
    "\n",
    "        # 3. 스케일드 닷 프로덕트 어텐션. 앞서 구현한 함수 사용.\n",
    "        # (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "        #     query         key.T          matmul_qk \n",
    "        # (1, 4, 40, 32)(1, 4, 32, 40) = (1, 4, 40, 40)\n",
    "        #   matmul_qk       value         scaled_attention\n",
    "        # (1, 4, 40, 40)(1, 4, 40, 32) = (1, 4, 40, 32)\n",
    "        scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n",
    "        # (batch_size, query의 문장 길이, num_heads, d_model/num_heads)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        # (1, 40, 4, 32)\n",
    "\n",
    "        # 4. 헤드 연결(concatenate)하기\n",
    "        # (batch_size, query의 문장 길이, d_model)\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "        # (1, 40, 128)\n",
    "\n",
    "        # 5. WO에 해당하는 밀집층 지나기\n",
    "        # (batch_size, query의 문장 길이, d_model)\n",
    "        outputs = self.dense(concat_attention)\n",
    "        # (1, 40, 128)(1, 128, 128)\n",
    "\n",
    "        return outputs  # (40, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, key의 문장 길이)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[[[0. 0. 0. 1. 1.]]]], shape=(1, 1, 1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(create_padding_mask(tf.constant([[1, 21, 777, 0, 0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(dff, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "    print('encoder_layer')\n",
    "    print(inputs)\n",
    "    \n",
    "    # 인코더는 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    \n",
    "    # 멀티-헤드 어텐션 (첫번째 서브층 / 셀프 어텐션)\n",
    "    attention = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention\")({\n",
    "            'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n",
    "            'mask': padding_mask # 패딩 마스크 사용\n",
    "        })\n",
    "\n",
    "    # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "    # 포지션 와이즈 피드 포워드 신경망 (두번째 서브층)\n",
    "    outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "    weights = model.get_weights()\n",
    "    print('model.get_weights()')\n",
    "    for i, a in enumerate(weights):\n",
    "        print(i, a.shape)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(vocab_size, num_layers, dff,\n",
    "            d_model, num_heads, dropout,\n",
    "            name=\"encoder\"):\n",
    "    print('encoder')\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    print(inputs)\n",
    "    # 인코더는 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # 포지셔널 인코딩 + 드롭아웃\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    # 인코더를 num_layers개 쌓기\n",
    "    for i in range(num_layers):\n",
    "      outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n",
    "          dropout=dropout, name=\"encoder_layer_{}\".format(i),\n",
    "      )([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x) # 패딩 마스크도 포함\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 1. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1.]\n",
      "   [0. 0. 1. 1. 1.]\n",
      "   [0. 0. 1. 0. 1.]\n",
      "   [0. 0. 1. 0. 0.]]]], shape=(1, 1, 5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(create_look_ahead_mask(tf.constant([[1, 2, 0, 4, 5]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer(dff, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "\n",
    "    # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 멀티-헤드 어텐션 (첫번째 서브층 / 마스크드 셀프 어텐션)\n",
    "    attention1 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "            'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n",
    "            'mask': look_ahead_mask # 룩어헤드 마스크\n",
    "        })\n",
    "\n",
    "    # 잔차 연결과 층 정규화\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "    # 멀티-헤드 어텐션 (두번째 서브층 / 디코더-인코더 어텐션)\n",
    "    attention2 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "            'query': attention1, 'key': enc_outputs, 'value': enc_outputs, # Q != K = V\n",
    "            'mask': padding_mask # 패딩 마스크\n",
    "        })\n",
    "\n",
    "    # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "    # 포지션 와이즈 피드 포워드 신경망 (세번째 서브층)\n",
    "    outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(vocab_size, num_layers, dff,\n",
    "            d_model, num_heads, dropout,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "\n",
    "    # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name='look_ahead_mask')\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 포지셔널 인코딩 + 드롭아웃\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    # 디코더를 num_layers개 쌓기\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n",
    "            dropout=dropout, name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size, num_layers, dff,\n",
    "                  d_model, num_heads, dropout,\n",
    "                  name=\"transformer\"):\n",
    "    print('transformer',vocab_size,num_layers,dff,d_model,num_heads)\n",
    "\n",
    "    # 인코더의 입력\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    print(inputs)\n",
    "\n",
    "    # 디코더의 입력\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    # 인코더의 패딩 마스크\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='enc_padding_mask')(inputs)\n",
    "\n",
    "    # 디코더의 룩어헤드 마스크(첫번째 서브층)\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask, output_shape=(1, None, None),\n",
    "        name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "    # 디코더의 패딩 마스크(두번째 서브층)\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='dec_padding_mask')(inputs)\n",
    "\n",
    "    # 인코더의 출력은 enc_outputs. 디코더로 전달된다.\n",
    "    enc_outputs = encoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n",
    "        d_model=d_model, num_heads=num_heads, dropout=dropout,\n",
    "    )(inputs=[inputs, enc_padding_mask]) # 인코더의 입력은 입력 문장과 패딩 마스크\n",
    "#     print('enc_outputs', enc_outputs)\n",
    "\n",
    "    # 디코더의 출력은 dec_outputs. 출력층으로 전달된다.\n",
    "    dec_outputs = decoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n",
    "        d_model=d_model, num_heads=num_heads, dropout=dropout,\n",
    "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    # 다음 단어 예측을 위한 출력층\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer 9000 4 512 128 4\n",
      "Tensor(\"inputs_193:0\", shape=(None, None), dtype=float32)\n",
      "encoder\n",
      "Tensor(\"inputs_194:0\", shape=(None, None), dtype=float32)\n",
      "encoder_layer\n",
      "Tensor(\"inputs_195:0\", shape=(None, None, 128), dtype=float32)\n",
      "MultiHeadAttention.__init__()\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"attention/strided_slice_138:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"inputs_195:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"attention/strided_slice_138:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"inputs_195:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"attention/dense_1096/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"attention/transpose_272:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "model.get_weights()\n",
      "0 (128, 128)\n",
      "1 (128,)\n",
      "2 (128, 128)\n",
      "3 (128,)\n",
      "4 (128, 128)\n",
      "5 (128,)\n",
      "6 (128, 128)\n",
      "7 (128,)\n",
      "8 (128,)\n",
      "9 (128,)\n",
      "10 (128, 512)\n",
      "11 (512,)\n",
      "12 (512, 128)\n",
      "13 (128,)\n",
      "14 (128,)\n",
      "15 (128,)\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"encoder_layer_0/attention/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"dropout_308/cond/Identity:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"encoder_layer_0/attention/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"dropout_308/cond/Identity:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"encoder_layer_0/attention/dense_1096/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"encoder_layer_0/attention/transpose_68:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "encoder_layer\n",
      "Tensor(\"inputs_196:0\", shape=(None, None, 128), dtype=float32)\n",
      "MultiHeadAttention.__init__()\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"attention/strided_slice_140:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"inputs_196:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"attention/strided_slice_140:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"inputs_196:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"attention/dense_1102/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"attention/transpose_276:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "model.get_weights()\n",
      "0 (128, 128)\n",
      "1 (128,)\n",
      "2 (128, 128)\n",
      "3 (128,)\n",
      "4 (128, 128)\n",
      "5 (128,)\n",
      "6 (128, 128)\n",
      "7 (128,)\n",
      "8 (128,)\n",
      "9 (128,)\n",
      "10 (128, 512)\n",
      "11 (512,)\n",
      "12 (512, 128)\n",
      "13 (128,)\n",
      "14 (128,)\n",
      "15 (128,)\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"encoder_layer_1/attention/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"encoder_layer_0/layer_normalization_341/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"encoder_layer_1/attention/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"encoder_layer_0/layer_normalization_341/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"encoder_layer_1/attention/dense_1102/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"encoder_layer_1/attention/transpose_68:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "encoder_layer\n",
      "Tensor(\"inputs_197:0\", shape=(None, None, 128), dtype=float32)\n",
      "MultiHeadAttention.__init__()\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"attention/strided_slice_142:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"inputs_197:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"attention/strided_slice_142:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"inputs_197:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"attention/dense_1108/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"attention/transpose_280:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "model.get_weights()\n",
      "0 (128, 128)\n",
      "1 (128,)\n",
      "2 (128, 128)\n",
      "3 (128,)\n",
      "4 (128, 128)\n",
      "5 (128,)\n",
      "6 (128, 128)\n",
      "7 (128,)\n",
      "8 (128,)\n",
      "9 (128,)\n",
      "10 (128, 512)\n",
      "11 (512,)\n",
      "12 (512, 128)\n",
      "13 (128,)\n",
      "14 (128,)\n",
      "15 (128,)\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"encoder_layer_2/attention/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"encoder_layer_1/layer_normalization_343/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"encoder_layer_2/attention/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"encoder_layer_1/layer_normalization_343/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"encoder_layer_2/attention/dense_1108/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"encoder_layer_2/attention/transpose_68:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "encoder_layer\n",
      "Tensor(\"inputs_198:0\", shape=(None, None, 128), dtype=float32)\n",
      "MultiHeadAttention.__init__()\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"attention/strided_slice_144:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"inputs_198:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"attention/strided_slice_144:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"inputs_198:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"attention/dense_1114/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"attention/transpose_284:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "model.get_weights()\n",
      "0 (128, 128)\n",
      "1 (128,)\n",
      "2 (128, 128)\n",
      "3 (128,)\n",
      "4 (128, 128)\n",
      "5 (128,)\n",
      "6 (128, 128)\n",
      "7 (128,)\n",
      "8 (128,)\n",
      "9 (128,)\n",
      "10 (128, 512)\n",
      "11 (512,)\n",
      "12 (512, 128)\n",
      "13 (128,)\n",
      "14 (128,)\n",
      "15 (128,)\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"encoder_layer_3/attention/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"encoder_layer_2/layer_normalization_345/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"encoder_layer_3/attention/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"encoder_layer_2/layer_normalization_345/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"encoder_layer_3/attention/dense_1114/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"encoder_layer_3/attention/transpose_68:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"encoder/encoder_layer_0/attention/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"encoder/dropout_308/cond/Identity:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"encoder/encoder_layer_0/attention/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"encoder/dropout_308/cond/Identity:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"encoder/encoder_layer_0/attention/dense_1096/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"encoder/encoder_layer_0/attention/transpose_68:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"encoder/encoder_layer_1/attention/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"encoder/encoder_layer_0/layer_normalization_341/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"encoder/encoder_layer_1/attention/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"encoder/encoder_layer_0/layer_normalization_341/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"encoder/encoder_layer_1/attention/dense_1102/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"encoder/encoder_layer_1/attention/transpose_68:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"encoder/encoder_layer_2/attention/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"encoder/encoder_layer_1/layer_normalization_343/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"encoder/encoder_layer_2/attention/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"encoder/encoder_layer_1/layer_normalization_343/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"encoder/encoder_layer_2/attention/dense_1108/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"encoder/encoder_layer_2/attention/transpose_68:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"encoder/encoder_layer_3/attention/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"encoder/encoder_layer_2/layer_normalization_345/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"encoder/encoder_layer_3/attention/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"encoder/encoder_layer_2/layer_normalization_345/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"encoder/encoder_layer_3/attention/dense_1114/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"encoder/encoder_layer_3/attention/transpose_68:0\", shape=(None, 4, None, 32), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadAttention.__init__()\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"attention_1/strided_slice_136:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"inputs_200:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"attention_1/strided_slice_136:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"inputs_200:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"attention_1/dense_1120/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"attention_1/transpose_272:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "MultiHeadAttention.__init__()\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"attention_2/strided_slice_136:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"layer_normalization_348/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"attention_2/strided_slice_136:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"encoder_outputs_86:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"attention_2/dense_1124/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"attention_2/transpose_272:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"decoder_layer_0/attention_1/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"dropout_317/cond/Identity:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"decoder_layer_0/attention_1/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"dropout_317/cond/Identity:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"decoder_layer_0/attention_1/dense_1120/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"decoder_layer_0/attention_1/transpose_68:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"decoder_layer_0/attention_2/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"decoder_layer_0/layer_normalization_348/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"decoder_layer_0/attention_2/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"encoder_outputs_85:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"decoder_layer_0/attention_2/dense_1124/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"decoder_layer_0/attention_2/transpose_68:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "MultiHeadAttention.__init__()\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"attention_1/strided_slice_138:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"inputs_201:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"attention_1/strided_slice_138:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"inputs_201:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"attention_1/dense_1130/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"attention_1/transpose_276:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "MultiHeadAttention.__init__()\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"attention_2/strided_slice_138:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"layer_normalization_351/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"attention_2/strided_slice_138:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"encoder_outputs_87:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"attention_2/dense_1134/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"attention_2/transpose_276:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"decoder_layer_1/attention_1/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"decoder_layer_0/layer_normalization_350/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"decoder_layer_1/attention_1/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"decoder_layer_0/layer_normalization_350/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"decoder_layer_1/attention_1/dense_1130/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"decoder_layer_1/attention_1/transpose_68:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"decoder_layer_1/attention_2/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"decoder_layer_1/layer_normalization_351/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"decoder_layer_1/attention_2/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"encoder_outputs_85:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"decoder_layer_1/attention_2/dense_1134/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"decoder_layer_1/attention_2/transpose_68:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "MultiHeadAttention.__init__()\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"attention_1/strided_slice_140:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"inputs_202:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"attention_1/strided_slice_140:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"inputs_202:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"attention_1/dense_1140/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"attention_1/transpose_280:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "MultiHeadAttention.__init__()\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"attention_2/strided_slice_140:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"layer_normalization_354/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"attention_2/strided_slice_140:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"encoder_outputs_88:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"attention_2/dense_1144/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"attention_2/transpose_280:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"decoder_layer_2/attention_1/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"decoder_layer_1/layer_normalization_353/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"decoder_layer_2/attention_1/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"decoder_layer_1/layer_normalization_353/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"decoder_layer_2/attention_1/dense_1140/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"decoder_layer_2/attention_1/transpose_68:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"decoder_layer_2/attention_2/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"decoder_layer_2/layer_normalization_354/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"decoder_layer_2/attention_2/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"encoder_outputs_85:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"decoder_layer_2/attention_2/dense_1144/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"decoder_layer_2/attention_2/transpose_68:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "MultiHeadAttention.__init__()\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"attention_1/strided_slice_142:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"inputs_203:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"attention_1/strided_slice_142:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"inputs_203:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"attention_1/dense_1150/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"attention_1/transpose_284:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "MultiHeadAttention.__init__()\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"attention_2/strided_slice_142:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"layer_normalization_357/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"attention_2/strided_slice_142:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"encoder_outputs_89:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"attention_2/dense_1154/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"attention_2/transpose_284:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"decoder_layer_3/attention_1/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"decoder_layer_2/layer_normalization_356/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"decoder_layer_3/attention_1/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"decoder_layer_2/layer_normalization_356/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before Tensor(\"decoder_layer_3/attention_1/dense_1150/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"decoder_layer_3/attention_1/transpose_68:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"decoder_layer_3/attention_2/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"decoder_layer_3/layer_normalization_357/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"decoder_layer_3/attention_2/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"encoder_outputs_85:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"decoder_layer_3/attention_2/dense_1154/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"decoder_layer_3/attention_2/transpose_68:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"decoder/decoder_layer_0/attention_1/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"decoder/dropout_317/cond/Identity:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"decoder/decoder_layer_0/attention_1/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"decoder/dropout_317/cond/Identity:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"decoder/decoder_layer_0/attention_1/dense_1120/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"decoder/decoder_layer_0/attention_1/transpose_68:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"decoder/decoder_layer_0/attention_2/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"decoder/decoder_layer_0/layer_normalization_348/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"decoder/decoder_layer_0/attention_2/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"encoder/encoder_layer_3/layer_normalization_347/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"decoder/decoder_layer_0/attention_2/dense_1124/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"decoder/decoder_layer_0/attention_2/transpose_68:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"decoder/decoder_layer_1/attention_1/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"decoder/decoder_layer_0/layer_normalization_350/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"decoder/decoder_layer_1/attention_1/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"decoder/decoder_layer_0/layer_normalization_350/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"decoder/decoder_layer_1/attention_1/dense_1130/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"decoder/decoder_layer_1/attention_1/transpose_68:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"decoder/decoder_layer_1/attention_2/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"decoder/decoder_layer_1/layer_normalization_351/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"decoder/decoder_layer_1/attention_2/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"encoder/encoder_layer_3/layer_normalization_347/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"decoder/decoder_layer_1/attention_2/dense_1134/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"decoder/decoder_layer_1/attention_2/transpose_68:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"decoder/decoder_layer_2/attention_1/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"decoder/decoder_layer_1/layer_normalization_353/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"decoder/decoder_layer_2/attention_1/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"decoder/decoder_layer_1/layer_normalization_353/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"decoder/decoder_layer_2/attention_1/dense_1140/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"decoder/decoder_layer_2/attention_1/transpose_68:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"decoder/decoder_layer_2/attention_2/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"decoder/decoder_layer_2/layer_normalization_354/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"decoder/decoder_layer_2/attention_2/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"encoder/encoder_layer_3/layer_normalization_347/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"decoder/decoder_layer_2/attention_2/dense_1144/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"decoder/decoder_layer_2/attention_2/transpose_68:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"decoder/decoder_layer_3/attention_1/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"decoder/decoder_layer_2/layer_normalization_356/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"decoder/decoder_layer_3/attention_1/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"decoder/decoder_layer_2/layer_normalization_356/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"decoder/decoder_layer_3/attention_1/dense_1150/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"decoder/decoder_layer_3/attention_1/transpose_68:0\", shape=(None, 4, None, 32), dtype=float32)\n",
      "MultiHeadAttention.call()\n",
      "batch_size= Tensor(\"decoder/decoder_layer_3/attention_2/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "query= Tensor(\"decoder/decoder_layer_3/layer_normalization_357/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "key= Tensor(\"decoder/decoder_layer_3/attention_2/strided_slice_34:0\", shape=(), dtype=int32)\n",
      "value= Tensor(\"encoder/encoder_layer_3/layer_normalization_347/batchnorm/add_1:0\", shape=(None, None, 128), dtype=float32)\n",
      "before Tensor(\"decoder/decoder_layer_3/attention_2/dense_1154/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "after Tensor(\"decoder/decoder_layer_3/attention_2/transpose_68:0\", shape=(None, 4, None, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "small_transformer = transformer(\n",
    "    vocab_size = 9000,\n",
    "    num_layers = 4,\n",
    "    dff = 512,\n",
    "    d_model = 128,\n",
    "    num_heads = 4,\n",
    "    dropout = 0.3,\n",
    "    name=\"small_transformer\")\n",
    "\n",
    "# weights=small_transformer.get_weights()\n",
    "# for a in weights:\n",
    "#     print(a.shape)\n",
    "    \n",
    "# tf.keras.utils.plot_model(\n",
    "#     small_transformer, to_file='small_transformer.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-8b64b1a3527d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Learning Rate\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Train Step\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Train Step'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Text' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "sample_learning_rate = CustomSchedule(d_model=128)\n",
    "\n",
    "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")\n",
    "Text(0.5, 0, 'Train Step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
